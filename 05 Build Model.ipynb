{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了使用模型，我们将输入数据传递给它。这将执行模型的forward，以及一些后台操作。不要直接调用model.forward() !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([6], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3, 28, 28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.Flatten  \n",
    "把2D的28x28图片转化为一个784个像素值的连续数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.Linear  \n",
    "使用其存储的权重和偏置值对输入进行一个线性变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.ReLU  \n",
    "非线性激活函数在模型的输入和输出之间创建复杂的映射，它们应用于线性变换后引入非线性，帮助神经网络学习各种各样的现象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[-2.6175e-01,  2.2003e-01, -2.8839e-01, -9.8895e-05, -3.2185e-02,\n",
      "          2.7835e-01,  3.5125e-01, -2.6175e-01, -2.6731e-01,  7.1005e-02,\n",
      "          3.0541e-01, -8.8769e-01,  9.9917e-02, -2.1116e-01,  3.0926e-01,\n",
      "         -1.7012e-01, -8.6330e-02, -6.9018e-01, -6.5629e-01, -3.9315e-01],\n",
      "        [-4.0696e-01,  6.6927e-02, -3.1480e-01, -1.2830e-02, -3.0320e-01,\n",
      "         -1.8560e-01,  3.7451e-02, -2.5285e-01, -1.5362e-01,  2.7031e-01,\n",
      "          4.3419e-01, -6.5314e-01,  9.9530e-02, -6.7920e-02,  1.3081e-01,\n",
      "         -2.0238e-01, -5.1028e-02, -5.7171e-01, -1.3553e-01,  2.1053e-01],\n",
      "        [-1.8415e-01, -6.7414e-02, -2.0869e-01,  1.5349e-01, -3.2052e-03,\n",
      "          3.8982e-02,  2.0596e-01, -3.4277e-01,  1.7266e-01,  6.9829e-02,\n",
      "          2.9444e-01, -6.9156e-01,  2.7304e-01, -1.6372e-01,  3.6503e-01,\n",
      "         -3.8827e-01, -1.7436e-01, -7.4231e-01, -2.4793e-01, -1.1239e-01]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0000, 0.2200, 0.0000, 0.0000, 0.0000, 0.2784, 0.3512, 0.0000, 0.0000,\n",
      "         0.0710, 0.3054, 0.0000, 0.0999, 0.0000, 0.3093, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.0669, 0.0000, 0.0000, 0.0000, 0.0000, 0.0375, 0.0000, 0.0000,\n",
      "         0.2703, 0.4342, 0.0000, 0.0995, 0.0000, 0.1308, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2105],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1535, 0.0000, 0.0390, 0.2060, 0.0000, 0.1727,\n",
      "         0.0698, 0.2944, 0.0000, 0.2730, 0.0000, 0.3650, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.Sequential  \n",
    "nn.Sequential是一个包含多个模块的有序容器。数据按照定义的顺序通过所有模块。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "\n",
    "input_image = torch.rand(3, 28, 28)\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.Softmax  \n",
    "神经网络最后一个线性层返回的原始logits值范围为负无穷到正无穷，softmax将他们压缩到[0,1]范围内，表示模型对每个类的预测概率，dim参数指明沿着该维度数值和为1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1108, 0.0733, 0.0697, 0.1166, 0.1079, 0.1052, 0.1151, 0.1317, 0.0796,\n",
      "         0.0900],\n",
      "        [0.1056, 0.0785, 0.0730, 0.1090, 0.1107, 0.1136, 0.1153, 0.1263, 0.0814,\n",
      "         0.0867],\n",
      "        [0.1033, 0.0826, 0.0675, 0.1195, 0.1141, 0.1085, 0.1090, 0.1287, 0.0854,\n",
      "         0.0813]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)\n",
    "print(pred_probab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0197, -0.0213, -0.0010,  ..., -0.0299,  0.0345,  0.0148],\n",
      "        [-0.0257,  0.0061,  0.0345,  ...,  0.0114, -0.0109,  0.0247]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([0.0315, 0.0021], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0045, -0.0386, -0.0414,  ..., -0.0225, -0.0259, -0.0352],\n",
      "        [-0.0144, -0.0240, -0.0143,  ..., -0.0370,  0.0009, -0.0321]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([0.0301, 0.0089], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0168, -0.0329,  0.0234,  ..., -0.0113,  0.0028, -0.0327],\n",
      "        [ 0.0131, -0.0364, -0.0385,  ..., -0.0397, -0.0150, -0.0285]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([ 0.0165, -0.0304], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "be203ce0b3afc4f5c37fbac412025d7ed1d67cabe9dd00b1fc8774c6d6d19d70"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
